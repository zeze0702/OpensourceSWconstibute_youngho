#문서 
!pip install --upgrade spacy
!pip install --upgrade spacy[cuda111,transformers]
!pip install jsonlines
!python -m spacy download en_core_web_lg
!python -m spacy download en_core_web_sm

!wget https://andrewhalterman.com/files/cleaned_masdar.jsonl
import jsonlines

from tqdm.autonotebook import tqdm
import jsonlines
import re

import spacy
from spacy import displacy
assert spacy.__version__ == "3.1.3"

nlp = spacy.load("en_core_web_lg")
nlp_sm = spacy.load("en_core_web_sm")

with jsonlines.open("cleaned_masdar.jsonl", "r") as f:
    articles = list(f.iter())

print(len(articles))

article = articles[313]
article

doc = nlp(article['body'])

dir(doc)

print(doc[5])


dir(doc[5])

displacy.render(doc, style="ent", jupyter=True)

spacy.explain("GPE")

just_text = [i['body'] for i in articles]
docs = list(tqdm(nlp.pipe(just_text), total=len(just_text)))

[(i.text, i.ent_iob_ + "-" + i.ent_type_) for i in doc[0:30]]

from collections import Counter

all_orgs = []
for d in docs:
    orgs = [ent.text for ent in d.ents if ent.label_ == "ORG"]
    all_orgs.extend(orgs)

Counter(all_orgs).most_common(15)

#title 저장 
negotiation_orgs = []
for d in docs:
    for ent in d.ents:
        if ent.label_ != "ORG":
            continue
        if re.search("negotiat|ceasefire|talks", ent.sent.text):
            negotiation_orgs.append(ent.text)
            
doc = nlp(articles[313]['body'])
sent = list(doc.sents)[1]
displacy.render(sent, style="dep", jupyter=True)

print(doc)
tok = doc[21]  # "Aleppo"
print(tok)


def loc_to_verb(tok):
    verb_phrase = []
    # first, iterate through all the ancesters of the token
    for i in tok.ancestors:
        # when you get to a verb (using a POS tag)...
        if i.pos_ == "VERB":
            # ...add the verb to the verb phrase list
            verb_phrase.append(i)
            # then, also add the direct object(s) of the verb, as long as the original token
            # is in the same subtree as the direct object
            verb_phrase.extend([j for j in i.children if j.dep_ == "dobj" and tok in i.subtree])
            # we only want the first verb, so stop after we find one
            break
    # expand out the verb phrase to get modifiers ("amod") of the direct object
    for i in verb_phrase:
        for j in i.children:
            if j.dep_ == "amod":
                verb_phrase.append(j)

    # sort the tokens by their position in the original sentence
    new_list = sorted(verb_phrase, key=lambda x: x.i)
    # join them together with the correct whitespace and return
    return ''.join([i.text_with_ws for i in new_list]).strip()

loc_to_verb(tok)

#문서 추출 후 제목 없는 파일 생성
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnbRX3/mXJZCarrT7EKcKU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zeze0702/OpensourceSWconstibute_youngho/blob/main/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZsYtbNxfj1v",
        "outputId": "386e2169-d8a6-4f57-8f70-87cdfcd07abc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (2.0.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (0.15.1+cu118)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.9/dist-packages (0.29.34)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch) (16.0.2)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch) (1.3.0)\n",
            "/bin/bash: -c: line 0: unexpected EOF while looking for matching `''\n",
            "/bin/bash: -c: line 1: syntax error: unexpected end of file\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision cython\n",
        "!pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI’"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "metadata": {
        "id": "mxJTu3q1fwUb"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://images.mypetlife.co.kr/content/uploads/2019/08/09153216/pets-3715733_1920-1024x682.jpg'\n",
        "urllib.request.urlretrieve(url,'cat_dog.jpg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7CjlKhEgkFc",
        "outputId": "6f211777-2541-4ccd-c38c-0163ce98bf40"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('cat_dog.jpg', <http.client.HTTPMessage at 0x7f1c14588310>)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img = cv2.imread('cat_dog.jgp')"
      ],
      "metadata": {
        "id": "73j-tnsYhZzv"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "jeEJGPAghtDB"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained =True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xb88_hZU5nm2",
        "outputId": "15fa137a-a534-4267-978b-b02cb2a7ae35"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n",
            "100%|██████████| 160M/160M [00:01<00:00, 140MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = T.Compose([\n",
        "    T.ToTensor()\n",
        "]\n",
        ")"
      ],
      "metadata": {
        "id": "pBTxxNs154TH"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = 'cat_dog.jpg'\n",
        "image = Image.open(image_path)\n",
        "image_tensor = transform(image)"
      ],
      "metadata": {
        "id": "heA0aCea6PXh"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  prediction = model([image_tensor])"
      ],
      "metadata": {
        "id": "GLhu5IBg6b72"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PD8-FNLE6vnb",
        "outputId": "48fc84c6-11cf-4c51-891b-810eda778d7d"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'boxes': tensor([[ 665.4686,  442.1950,  920.0180,  638.0698],\n",
            "        [ 458.8671,  193.8185,  691.0240,  643.5814],\n",
            "        [ 132.2447,  326.7481,  306.2527,  638.2285],\n",
            "        [ 222.0401,  260.9561,  457.4732,  646.0949],\n",
            "        [ 459.7807,  194.3062,  684.8488,  651.2650],\n",
            "        [ 157.3245,  253.6629,  469.9864,  644.3327],\n",
            "        [ 268.8198,  271.0645,  462.8854,  644.0151],\n",
            "        [ 128.9978,  332.0967,  319.1451,  628.6864],\n",
            "        [ 471.9638,  201.8055,  881.1995,  638.6366],\n",
            "        [   0.0000,  434.1219, 1024.0000,  677.7223],\n",
            "        [ 125.7631,  374.2219,  897.5781,  649.5337],\n",
            "        [  78.0130,  192.4376,  776.8642,  673.3724],\n",
            "        [ 239.4230,  201.5661,  662.6106,  634.9756],\n",
            "        [ 567.9014,  353.9862,  912.3868,  628.8291]]), 'labels': tensor([17, 18, 17, 18, 17, 17, 88, 18, 17, 81, 17, 63, 18, 17]), 'scores': tensor([0.9974, 0.9833, 0.9831, 0.9366, 0.8544, 0.4928, 0.3492, 0.3026, 0.1357,\n",
            "        0.0650, 0.0614, 0.0607, 0.0532, 0.0519])}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KqkJmR7s6z-G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}

#텍스트 유사도 분석 
import numpy as np
from numpy import dot
from numpy.linalg import norm

def cos_sim(A, B):
  return dot(A, B)/(norm(A)*norm(B))

doc1 = np.array([0,1,1,1])
doc2 = np.array([1,0,1,1])
doc3 = np.array([2,0,2,2])

print('문서 1과 문서2의 유사도 :',cos_sim(doc1, doc2))
print('문서 1과 문서3의 유사도 :',cos_sim(doc1, doc3))
print('문서 2와 문서3의 유사도 :',cos_sim(doc2, doc3))'

###
문서 1과 문서2의 유사도 : 0.67
문서 1과 문서3의 유사도 : 0.67
문서 2과 문서3의 유사도 : 1.00
###

#보고서 시스템(데모ver)
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

data = pd.read_csv('movies_metadata.csv', low_memory=False)
data.head(2)

#이미지 유사도 분석
#ORB
import numpy as np
import cv2
from matplotlib import pyplot as plt
import glob
import os

img_list = glob.glob('/home/john/PycharmProjects/MOT_test/person_frame/train/*.jpg')
# dir_img_list = glob.glob('/home/john/PycharmProjects/MOT_test/person_frame/*.jpg')


for i in range(len(img_list[:20])):
    img1 = cv2.imread(f'{img_list[7]}',0)          # queryImage
    img2 = cv2.imread(f'{img_list[i]}',0) # trainImage

    orb = cv2.ORB_create()
    kp1, des1 = orb.detectAndCompute(img1,None)
    kp2, des2 = orb.detectAndCompute(img2,None)
    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)
    try:
        matches = bf.match(des1,des2)
        matches = sorted(matches, key = lambda x:x.distance)
        img3 = cv2.drawMatches(img1,kp1,img2,kp2,matches[:10],None,flags=2)
        plt.imshow(img3),plt.show()

    except:
        print('유사도 없음')
        
#SIFT     
import numpy as np
import cv2
from matplotlib import pyplot as plt
import glob
import os


img_list = glob.glob('/home/john/PycharmProjects/MOT_test/person_frame/train/*.jpg')
# dir_img_list = glob.glob('/home/john/PycharmProjects/MOT_test/person_frame/*.jpg')

for i in range(len(img_list)):
    img1 = cv2.imread(f'{img_list[0]}',0)          # queryImage
    img2 = cv2.imread(f'{img_list[i]}',0) # trainImage

    sift = cv2.xfeatures2d.SIFT_create()
    kp1, des1 = sift.detectAndCompute(img1,None)
    kp2, des2 = sift.detectAndCompute(img2,None)
    bf = cv2.BFMatcher()

    matches = bf.knnMatch(des1,des2, k=2)
    good = []

    for m,n in matches:
        # if m.distance < 0.3*n.distance:
        if m.distance < n.distance:

            good.append([m])
    print('same_good', good)
    img3 = cv2.drawMatchesKnn(img1,kp1,img2,kp2,good,None,flags=2)
    plt.imshow(img3),plt.show()
    cv2.imshow("comapre", img3)
    cv2.waitKey(1) 
    

#auto incoder
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.init as init
import torchvision.datasets as dset
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import logging
import sys

flag = 0 # if 1 train  0 test

logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)
logger.addHandler(logging.StreamHandler(sys.stdout))

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(device)
mnist_train = dset.MNIST("./", train=True, transform=transforms.ToTensor(), target_transform=None, download=True)
mnist_test = dset.MNIST("./", train=False, transform=transforms.ToTensor(), target_transform=None, download=True)

class Encoder(nn.Module):
  def __init__(self, latency_num):
      super(Encoder, self).__init__()
      self.layer1 = nn.Sequential(
          nn.Conv2d(1, 16, 3, padding=1),  # batch x 16 x 28 x 28
          nn.ReLU(),
          nn.BatchNorm2d(16),
          nn.Conv2d(16, 32, 3, padding=1),  # batch x 32 x 28 x 28
          nn.ReLU(),
          nn.BatchNorm2d(32),
          nn.Conv2d(32, 64, 3, padding=1),  # batch x 64 x 28 x 28
          nn.ReLU(),
          nn.BatchNorm2d(64),
          nn.MaxPool2d(2, 2)  # batch x 64 x 14 x 14
      )
      self.layer2 = nn.Sequential(
          nn.Conv2d(64, 128, 3, padding=1),  # batch x 128 x 14 x 14
          nn.ReLU(),
          nn.BatchNorm2d(128),
          nn.MaxPool2d(2, 2),
          nn.Conv2d(128, 256, 3, padding=1),  # batch x 256 x 7 x 7
          nn.ReLU()
      )
      self.fc1 = nn.Linear(256 * 7 * 7, latency_num)

      self.hidden2mu = nn.Linear(1024,1024)
      self.hidden2log_var = nn.Linear(1024,1024)
      self.relu = nn.ReLU()

  def forward(self, x):
      out = self.layer1(x)
      out = self.layer2(out)
      out = out.view(batch_size, -1)
      out = self.fc1(out)
      # mu = self.hidden2mu(out)
      # mu = self.relu(mu)
      # log_var = self.hidden2log_var(out)
      # log_var = self.relu(log_var)
      #
      # std = torch.exp(0.5*log_var)
      # eps = torch.randn_like(std)
      #out = mu + eps*std
      return out



class Decoder(nn.Module):
  def __init__(self,latency_num):
      super(Decoder, self).__init__()
      self.layer1 = nn.Sequential(
          nn.ConvTranspose2d(256, 128, 3, 2, 1, 1),  # batch x 128 x 14 x 14
          nn.ReLU(),
          nn.BatchNorm2d(128),
          nn.ConvTranspose2d(128, 64, 3, 1, 1),  # batch x 64 x 14 x 14
          nn.ReLU(),
          nn.BatchNorm2d(64)
      )
      self.layer2 = nn.Sequential(
          nn.ConvTranspose2d(64, 16, 3, 1, 1),  # batch x 16 x 14 x 14
          nn.ReLU(),
          nn.BatchNorm2d(16),
          nn.ConvTranspose2d(16, 1, 3, 2, 1, 1),  # batch x 1 x 28 x 28
          nn.ReLU()
      )
      self.fc1 = nn.Linear(latency_num, 256 * 7 * 7)

  def forward(self, x):
      out = self.fc1(x)
      out = out.view(batch_size, 256, 7, 7)
      out = self.layer1(out)
      out = self.layer2(out)
      return out

def cosin_metric(x1, x2):
  return np.dot(x1, x2) / (np.linalg.norm(x1) * np.linalg.norm(x2))

def loss_function(recon_x, x, mu, logvar):
  BCE = nn.functional.binary_cross_entropy(recon_x, x)

  # see Appendix B from VAE paper:
  # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014
  # https://arxiv.org/abs/1312.6114
  # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)
  KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
  #kl_loss = - 0.5 * K.mean(1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma), axis=-1)

  return BCE + KLD

def discrete_cmap(N, base_cmap=None):
  """Create an N-bin discrete colormap from the specified input map"""

  # Note that if base_cmap is a string or None, you can simply do
  #    return plt.cm.get_cmap(base_cmap, N)
  # The following works for string, None, or a colormap instance:

  base = plt.cm.get_cmap(base_cmap)
  color_list = base(np.linspace(0, 1, N))
  cmap_name = base.name + str(N)
  return base.from_list(cmap_name, color_list, N)

if __name__ == "__main__":
  avg_list = []
  min_list=[]
  max_list = []
  var_list=[]

  for latency_num in range(2,20):

      learning_rate = 0.0002
      num_epoch = 5
      batch_size = 256
      train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=2,
                                                 drop_last=True)

      #     loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=0)
      test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=2,
                                                drop_last=True)
      encoder = Encoder(latency_num=latency_num).to(device)
      decoder = Decoder(latency_num=latency_num).to(device)

      #train
      if flag==1:
          # 인코더 디코더의 파라미터를 동시에 학습시키기 위해
          parameters = list(encoder.parameters()) + list(decoder.parameters())
          loss_func = nn.MSELoss()

          optimizer = torch.optim.Adam(parameters, lr=learning_rate)

          for epoch in range(num_epoch):
              train_loss = 0
              for batch_idx, [image, label] in enumerate(train_loader):
                  optimizer.zero_grad()
                  image = image.to(device)

                  output= encoder(image)
                  output = decoder(output)
                  loss = loss_func(output, image)
                  loss.backward()
                  train_loss += loss.item()
                  optimizer.step()

                  if batch_idx % 10 == 0:
                      logger.info('Train Epoch: {}/{} [{}/{} ({:.0f}%)] Loss: {:.6f}'.format(
                          epoch,num_epoch, batch_idx * len(image), len(train_loader.sampler),
                                 100. * batch_idx / len(train_loader), loss.item()))
              print('====> Epoch: {} Average loss: {:.4f}'.format(
                  epoch, train_loss / len(train_loader.dataset)))

                  # out_img = torch.squeeze(output.cpu().data)
                  # print(out_img.size())
                  # plt.subplot(1, 2, 1)
                  # plt.imshow(torch.squeeze(image[0]).cpu().numpy(), cmap='gray')
                  # plt.subplot(1, 2, 2)
                  # plt.imshow(out_img[0].numpy(), cmap='gray')
                  # plt.show()

          torch.save(encoder.cpu().state_dict(), f'./encoder_model/{latency_num}_encoder.pt')
          torch.save(decoder.cpu().state_dict(), f'./encoder_model/{latency_num}_decoder.pt')
          # train 결과 체크

      # test
      if  flag==0 :
          tt = []
          all = []
          encoder.load_state_dict(torch.load(f'./encoder_model/{latency_num}_encoder.pt'))
          decoder.load_state_dict(torch.load(f'./encoder_model/{latency_num}_decoder.pt'))
          encoder.eval()
          decoder.eval()
          with torch.no_grad():
              for j, [image, label] in enumerate(test_loader):
                  image = image.to(device)

                  outputs = encoder(image)
                  output = decoder(outputs)

                  out_img = torch.squeeze(output.cpu().data)

                  # plt.subplot(1, 2, 1)
                  # plt.imshow(torch.squeeze(image[0]).cpu().numpy(), cmap='gray')
                  # plt.subplot(1, 2, 2)
                  # plt.imshow(out_img[0].numpy(), cmap='gray')
                  # plt.show()
                  N = 10
                  plt.figure(figsize=(6, 6))
                  plt.scatter(outputs[:, 0].cpu().numpy(), outputs[:, 1].cpu().numpy(), c=label, marker='o',
                              edgecolor='none', cmap=discrete_cmap(N, 'jet'))
                  plt.colorbar(ticks=range(N))
                  plt.show()

                  for idx, la in enumerate(label) :
                      if la == 1:
                          latent_ = outputs[idx].view(-1).cpu().numpy()
                          tt.append(latent_)
                      else :
                          latent_s = outputs[idx].view(-1).cpu().numpy()
                          all.append(latent_s)



          tot = []
          for tt_idx in range(len(tt)-1):
              temp = cosin_metric(tt[0],tt[tt_idx+1])
              tot.append(temp)
              print(temp)
          print("avg = ",sum(tot, 0.0)/len(tot))
          print("min = ",min(tot))
          print("max = ",max(tot))
          print("total = ",len(tot))
          avg_list.append(sum(tot, 0.0)/len(tot))
          min_list.append(min(tot))
          max_list.append(max(tot))
          var_list.append(np.var(tot))

  print(avg_list.index(max(avg_list)))
  print(avg_list.index(min(avg_list)))
  print(min_list.index(min(avg_list)))
  print(max_list.index(max(avg_list)))
  print(var_list.index(max(avg_list)))

  # with open('./latent_v', 'wb') as f:
          #     f.write(tt)

#xml 파일 저장
from xml.etree.ElementTree import Element, SubElement, ElementTree


root = Element("Countries")
element1 = Element("Korea")
root.append(element1)

sub_element1 = SubElement(element1, "City")
sub_element1.text = "Seoul"

element2 = Element("Japanese")
root.append(element2)

sub_element2 = SubElement(element2, "City")
sub_element2.text = "Tokyo"

tree = ElementTree(root)

fileName = "example.xml"
with open(fileName, "wb") as file:
    tree.write(file, encoding='utf-8', xml_declaration=True)
    
root = Element("Countries")
element1 = Element("Korea")
root.append(element1)

sub_element1 = SubElement(element1, "City")
sub_element1.text = "Seoul"
sub_element1.set("attr1", "value1")
sub_element1.set("attr2", "value2")

element2 = Element("Japanese")
root.append(element2)

sub_element2 = SubElement(element2, "City")
sub_element2.text = "Tokyo"
sub_element2.set("attr1", "value1")
sub_element2.set("attr2", "value2")

tree = ElementTree(root)

fileName = "example.xml"
with open(fileName, "wb") as file:
    tree.write(file, encoding='utf-8', xml_declaration=True)  

def _pretty_print(current, parent=None, index=-1, depth=0):
    for i, node in enumerate(current):
        _pretty_print(node, current, i, depth + 1)
    if parent is not None:
        if index == 0:
            parent.text = '\n' + ('\t' * depth)
        else:
            parent[index - 1].tail = '\n' + ('\t' * depth)
        if index == len(parent) - 1:
            current.tail = '\n' + ('\t' * (depth - 1))


filePath = "./example.xml"
tree = ET.parse(filePath)

_pretty_print(tree.getroot())

ET.dump(tree)

